"""
RAG ì²´ì¸ êµ¬ì„± ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ RAG(Retrieval-Augmented Generation) ì²´ì¸ì„ êµ¬ì„±í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
ì£¼ìš” ê¸°ëŠ¥:
- í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
- LLM ì„¤ì •
- ê²€ìƒ‰ê¸° êµ¬ì„±
- RAG ì²´ì¸ ìƒì„±

ì„¤ê³„ ì² í•™:
- ì´ ëª¨ë“ˆì€ LangChainì˜ ConversationalRetrievalChainì„ í™œìš©í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
- ë¹Œë” íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ ì²´ì¸ êµ¬ì„± ê³¼ì •ì„ ì¶”ìƒí™”í•˜ê³  ìœ ì—°í•˜ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
- ì™¸ë¶€ ì˜ì¡´ì„±(í”„ë¡¬í”„íŠ¸, LLM)ì„ ìº¡ìŠí™”í•˜ì—¬ ê´€ë¦¬í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
"""
import logging
from typing import Dict, List, Optional

from langchain_community.vectorstores import Chroma
from langchain.llms.base import BaseLLM
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.chat_history import BaseChatMessageHistory

from src.rag_example.config.settings import OLLAMA_MODEL, SEARCH_K
from src.rag_example.pipeline.querying.prompts import get_condense_prompt, get_qa_prompt, PromptTemplate
from src.rag_example.pipeline.querying.llm_factory import LLMFactory
from src.rag_example.pipeline.summarizing_memory import SummarizingMemory

logger = logging.getLogger(__name__)


class RAGChainBuilder:
    """
    RAG ì²´ì¸ êµ¬ì„±ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
    - LLM ì„¤ì •
    - ê²€ìƒ‰ê¸° êµ¬ì„±
    - RAG ì²´ì¸ ìƒì„±
    
    ì„¤ê³„ ì˜ë„:
    - ë¹Œë” íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ë³µì¡í•œ RAG ì²´ì¸ êµ¬ì„± ê³¼ì •ì„ ì¶”ìƒí™”í•©ë‹ˆë‹¤.
    - ê° êµ¬ì„± ìš”ì†Œ(LLM, ë©”ëª¨ë¦¬, í”„ë¡¬í”„íŠ¸, ê²€ìƒ‰ê¸°)ë¥¼ ëª¨ë“ˆí™”í•˜ì—¬ ê´€ë¦¬í•˜ê¸° ì‰½ê²Œ í•©ë‹ˆë‹¤.
    - íŒŒì´í”„ë¼ì¸ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ë¡œ, ì‚¬ìš©ì ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
    - LangChainì˜ ì¶”ìƒí™”ë¥¼ í™œìš©í•˜ë©´ì„œë„ í•„ìš”í•œ ê²½ìš° ë‚´ë¶€ì ìœ¼ë¡œ êµ¬ì²´ì ì¸ êµ¬í˜„ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, 
                 llm_type: str = "ollama",
                 model_name: str = OLLAMA_MODEL,
                 search_k: int = SEARCH_K):
        """
        RAGChainBuilder ì´ˆê¸°í™”
        
        Args:
            llm_type: ì‚¬ìš©í•  LLM íƒ€ì… ("ollama" ë˜ëŠ” "claude")
            model_name: ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„
            search_k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜
        """
        self.llm_type = llm_type
        self.model_name = model_name
        self.search_k = search_k
        self.llm = None
        self.chain = None
        
        # ì„¸ì…˜ë³„ ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
        self.session_histories = {}
    
    def _create_llm(self) -> BaseLLM:
        """
        LLMì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ìƒì„±ëœ LLM ê°ì²´
            
        ì„¤ê³„ ì°¸ê³ :
            ì´ ë©”ì„œë“œëŠ” íŒ©í† ë¦¬ íŒ¨í„´ì„ ì ìš©í•˜ì—¬ LLM ìƒì„±ì„ ìº¡ìŠí™”í•©ë‹ˆë‹¤.
            LLMFactoryë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ LLMì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            í˜„ì¬ëŠ” Ollamaì™€ Claudeë¥¼ ì§€ì›í•˜ë©°, í•„ìš”ì— ë”°ë¼ LLMFactoryì— ë‹¤ë¥¸ LLMì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        logger.info(f"LLM ìƒì„±: {self.llm_type} - {self.model_name}")
        
        try:
            return LLMFactory.create_llm(self.llm_type, self.model_name, temperature=0.1)
        except ValueError as e:
            logger.error(f"LLM ìƒì„± ì‹¤íŒ¨: {str(e)}")
            logger.info(f"ê¸°ë³¸ Ollama LLMìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return LLMFactory.create_llm("ollama", OLLAMA_MODEL, temperature=0.1)
    
    def get_session_history(self, session_id: str) -> BaseChatMessageHistory:
        """
        ì„¸ì…˜ IDì— í•´ë‹¹í•˜ëŠ” ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            session_id: ì„¸ì…˜ ì•„ì´ë””
            
        Returns:
            í•´ë‹¹ ì„¸ì…˜ì˜ BaseChatMessageHistory ê°ì²´
        """
        # ì„¸ì…˜ IDê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if session_id not in self.session_histories:
            logger.info(f"ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
            # ìš”ì•½ì„ ìœ„í•œ LLM ì‚¬ìš©
            if self.llm is None:
                self.llm = self._create_llm()
            self.session_histories[session_id] = SummarizingMemory(
                session_id=session_id,
                llm=self.llm,
                max_recent_turns=4  # ìµœê·¼ 4í„´ì˜ ëŒ€í™”ë§Œ ìœ ì§€
            )
        
        return self.session_histories[session_id]
    
    def _create_prompt_templates(self) -> Dict[str, PromptTemplate]:
        """
        í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Returns:
            ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë”•ì…”ë„ˆë¦¬
        """
        # ì™¸ë¶€ ëª¨ë“ˆì—ì„œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
        condense_prompt = get_condense_prompt()
        qa_prompt = get_qa_prompt()
        
        return {
            "condense": condense_prompt,
            "qa": qa_prompt
        }
    
    def build(self, vectorstore: Chroma) -> Optional[callable]:
        """
        RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
        
        Args:
            vectorstore: ì‚¬ìš©í•  ë²¡í„° ì €ì¥ì†Œ
            
        Returns:
            êµ¬ì„±ëœ ì²´ì¸ í•¨ìˆ˜
        """
        logger.info("RAG ì²´ì¸ ìƒì„± ì‹œì‘...")
        
        # LLM ìƒì„±
        if self.llm is None:
            self.llm = self._create_llm()
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
        prompts = self._create_prompt_templates()
        qa_prompt = prompts["qa"]
        
        # ê²€ìƒ‰ê¸° êµ¬ì„±
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.search_k}
        )
        
        def format_docs(docs):
            return "\n\n".join([doc.page_content for doc in docs])
            
        # ê¸°ë³¸ RAG ì²´ì¸ êµ¬ì„± - ë‹¨ìˆœí™”
        def run_rag_chain(query_text, history=None):
            # 1. ë¬¸ì„œ ê²€ìƒ‰
            docs = retriever.invoke(query_text)
            context = format_docs(docs)
            
            # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
            chat_history = ""
            if history and isinstance(history, SummarizingMemory):
                # ìš”ì•½ëœ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
                chat_history = history.load_summary_and_recent()
                if chat_history:
                    logger.debug(f"ëŒ€í™” ê¸°ë¡ ì‚¬ìš© (ê¸¸ì´: {len(chat_history)})")
            
            # 2. í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë° LLM ìš”ì²­
            prompt_args = {"context": context, "question": query_text, "chat_history": chat_history}
            chain_response = qa_prompt.format_messages(**prompt_args)
            llm_response = self.llm.invoke(chain_response)
            
            # LLM íŒ©í† ë¦¬ì˜ ì‘ë‹µ ì²˜ë¦¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë³„ ì‘ë‹µ í˜•ì‹ ì°¨ì´ ì²˜ë¦¬
            return LLMFactory.process_response(self.llm_type, llm_response)
        
        # ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
        def process_with_history(inputs, config=None):
            if config is None:
                config = {}
                
            session_id = config.get("configurable", {}).get("session_id", "default")
            history = self.get_session_history(session_id)
            query = ""
            
            try:
                # ì§ˆë¬¸ ì¶”ì¶œ
                query = inputs.get("question", "")
                if not query:
                    return "ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
                
                # ëŸ°ë„ˆë¸” í•¨ìˆ˜ ì‹¤í–‰ - ëŒ€í™” ê¸°ë¡ ì „ë‹¬
                result = run_rag_chain(query, history)
                
                # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ AI ì‘ë‹µ ì¶”ê°€
                history.add_messages([
                    HumanMessage(content=query),
                    AIMessage(content=result)
                ])
                
                return result
            except Exception as e:
                logger.error(f"RAG ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ì§ˆë¬¸: {query}, ì„¸ì…˜ ID: {session_id}")
                
                # ì˜ˆì™¸ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
                error_message = f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                return error_message
        
        # ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ë¥¼ ê´€ë¦¬í•˜ëŠ” í•¨ìˆ˜ ì„¤ì •
        self.chain = process_with_history
        
        logger.info("RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
        
        return self.chain
    
    def run(self, query: str, session_id: str = "default") -> str:
        """
        RAG ì²´ì¸ì„ ì‹¤í–‰í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID, ê¸°ë³¸ê°’ì€ "default"
            
        Returns:
            ë‹µë³€ ë¬¸ìì—´
        """
        if self.chain is None:
            logger.error("RAG ì²´ì¸ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build() ë©”ì„œë“œë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            return "ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        try:
            # ì„¸ì…˜ í™•ì¸ - ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œë§Œ ìƒˆë¡œ ìƒì„±
            if session_id not in self.session_histories:
                logger.info(f"ìƒˆ ì„¸ì…˜ ìƒì„±: {session_id}")
                self.session_histories[session_id] = SummarizingMemory(session_id=session_id)
            else:
                logger.info(f"ê¸°ì¡´ ì„¸ì…˜ ì‚¬ìš©: {session_id}, ë©”ì‹œì§€ ìˆ˜: {len(self.session_histories[session_id].messages)}")
            
            # ì²´ì¸ ì‹¤í–‰
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: '{query}', ì„¸ì…˜ ID: {session_id}")
            result = self.chain(
                {"question": query}, 
                config={"configurable": {"session_id": session_id}}
            )
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ: '{query}'")
            return result
            
        except Exception as e:
            logger.error(f"RAG ì²´ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def reset_memory(self, session_id: str = "default") -> None:
        """
        ì„¸ì…˜ì˜ ë©”ì„¸ì§€ íˆìŠ¤í† ë¦¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.

        Args:
            session_id: ì´ˆê¸°í™”í•  ì„¸ì…˜ ID, "all" ì§€ì • ì‹œ ì „ì²´ ì´ˆê¸°í™”
        """
        if session_id == "all":
            self.session_histories.clear()
            logger.info("âœ… ëª¨ë“  ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
            return

        if session_id in self.session_histories:
            self.session_histories[session_id].clear()
            logger.info(f"âœ… ì„¸ì…˜ '{session_id}' íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            # ì„¸ì…˜ì´ ì—†ë‹¤ë©´ ìƒˆë¡œ ìƒì„±
            if self.llm is None:
                self.llm = self._create_llm()
            self.session_histories[session_id] = SummarizingMemory(
                session_id=session_id,
                llm=self.llm,
            max_recent_turns=4
        )
        logger.info(f"ğŸ†• ìƒˆ ì„¸ì…˜ '{session_id}' ìƒì„± ë° ì´ˆê¸°í™” ì™„ë£Œ")
